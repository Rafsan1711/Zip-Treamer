# üéØ GambitFlow Synapse-Base: Complete Master Plan

## üìã Executive Summary

This is a comprehensive reconstruction plan for Synapse-Base, a lightweight yet powerful chess engine designed for:
- Ultra-low latency inference (HF Spaces: 2 vCPU, 16GB RAM)
- Opening mastery through statistical databases
- Tactical brilliance via puzzle training
- Endgame perfection using Syzygy tablebases
- Continuous self-improvement through automated self-play and fine-tuning

**Critical Discovery from Analysis:**
- Current opening notebook has logical flaws in data aggregation
- Puzzle database is sufficient but needs better integration
- Endgame data collection is adequate (600K positions total)
- Failed Synapse-Base used wrong architecture (119 channels without proper data)
- Nano's architecture is a better starting point than designing from scratch

---

## üîç Deep Analysis of Existing Notebooks

### 1. Opening Architect Issues ‚ùå

**Problems Found:**
```python
# Current code from Cell 3
for fen, stat_data in stats.items():
    turn = fen.split(' ')[1]
    for move_san, results in stat_data['moves'].items():
        total = results['white'] + results['black'] + results['draw']
        score = (results['white'] - results['black']) / total
```

**Critical Flaws:**
1. **Turn Context Loss**: Score calculation doesn't consider whose turn it is
   - If it's Black's turn but we calculate `(white_wins - black_wins)`, the sign is wrong
   - White winning from Black's position means the move was BAD for Black
   
2. **Move Popularity Ignored**: No frequency threshold
   - Rare moves (played once) get same weight as main lines (played 1000 times)
   
3. **No Continuation Depth**: Only first-order statistics
   - Doesn't track "after e4 e5 Nf3, what happens?"
   - No transposition handling

4. **ELO Filter Too Low**: 2000+ includes amateur games
   - Should be 2500+ minimum for opening theory

**Status**: Needs complete redesign ‚úÖ

### 2. Tactical Forge Analysis ‚úÖ

**Strengths:**
- Rating filter 2000+ with popularity 90+ is good
- 500K+ puzzles collected
- Themes are preserved in database

**Weaknesses:**
- Not integrated into training (just stored in DB)
- No puzzle difficulty stratification
- Missing: Fork, Pin, Skewer explicit labels

**Status**: Good foundation, needs training integration ‚úÖ

### 3. Endgame Oracle Analysis ‚úÖ

**Strengths:**
- Proper Syzygy tablebase usage
- Distributed generation (3 workers √ó 200K = 600K positions)
- Covers 3-4-5 piece endgames

**Weaknesses:**
- Only WDL (Win/Draw/Loss), no DTZ (Distance to Zero)
- No 6-piece endgames
- Could benefit from more positions in critical scenarios

**Status**: Adequate but can be enhanced ‚úÖ

### 4. Training Data (Nexus-Core) Analysis ‚úÖ

**Current Setup:**
- 5M games from Lichess 2017-01
- MIN_ELO = 2000
- Processes first 20 moves

**Issues:**
- 2000 ELO too low for engine training
- 2017 games miss modern theory
- Only opening phase covered, middlegame ignored

**Status**: Needs upgrade to 2500+ ELO and recent games ‚úÖ

### 5. Model Architecture Analysis

**Nexus-Nano** (16.5 MB ONNX):
```python
# Simple CNN
Input: (12, 8, 8)
Conv ‚Üí BN ‚Üí ReLU
Residual Tower (10 blocks)
Value Head: Conv ‚Üí Flatten ‚Üí FC ‚Üí Tanh
```
- Works well for tactics
- Fast inference
- Limited strategic depth

**Nexus-Core** (13.34 MB ONNX):
```python
# Enhanced ResNet
Input: (12, 8, 8)
ResNet Tower (10 blocks, 128 filters)
Value Head only
```
- Better endgame understanding
- Still lacks global reasoning

**Failed Synapse-Base** (145 MB ONNX):
```python
# Over-engineered hybrid
Input: (119, 8, 8)  # ‚ùå Wrong!
CNN + Transformer
38M parameters
```
**Why it failed:**
- Feature extractor created 119 channels but **no training data used them**
- Transformer added without proper input formatting
- Too heavy for intended deployment (145MB is 10x Nano)

---

## üéØ Synapse-Base Design Philosophy

### Core Principles

1. **Lightweight Yet Mighty**: Target 20-30 MB ONNX
2. **Fast Inference**: <100ms per position on 2 vCPU
3. **Knowledge Distillation**: Learn from curated data, not raw games
4. **Specialized Heads**: Separate reasoning for opening/middle/endgame
5. **Self-Improving**: Automated fine-tuning from self-play

### Why Not Fine-Tune Nano Directly?

**Pros of Fine-Tuning Nano:**
- Already trained on tactics
- Proven architecture
- Fast to implement

**Cons (Why We Need New Architecture):**
- Nano lacks opening knowledge integration
- No endgame tablebase reasoning
- Single value head insufficient for multi-phase chess
- Can't add new input channels without breaking pretrained weights

**Decision**: Build new architecture inspired by Nano but enhanced ‚úÖ

---

## üèóÔ∏è Architecture Design: Synapse-Base

### Input Representation (Optimized)

```python
# Total: 28 channels (not 119!)
# 
# A. Board State (12 channels)
# - Pieces: 6 white (P,N,B,R,Q,K) + 6 black
#
# B. Game Context (8 channels)
# - Turn (1): all 1.0 if white's turn
# - Castling rights (4): KQkq
# - En passant file (8): one-hot encoding
#
# C. Move History (8 channels)
# - Last 4 moves encoded as from-to heatmaps
#   (2 channels per move: from_square, to_square)
```

**Why 28 channels?**
- Sufficient information for strong play
- 4.6x smaller than failed 119-channel design
- Fast convolution operations
- Proven in AlphaZero-style engines

### Network Architecture

```python
class SynapseBase(nn.Module):
    # Input: (28, 8, 8)
    
    # 1. Spatial Feature Extraction
    conv_stem: 28 ‚Üí 128 filters, 3√ó3
    
    # 2. Residual Tower (15 blocks, not 20)
    # Each block: Conv3√ó3 ‚Üí BN ‚Üí ReLU ‚Üí Conv3√ó3 ‚Üí BN ‚Üí Add ‚Üí ReLU
    residual_tower: 15 √ó ResBlock(128)
    
    # 3. Multi-Head Output
    
    # 3a. Value Head (Position Evaluation)
    value_head:
        Conv 128‚Üí32, 1√ó1
        Flatten ‚Üí FC 2048‚Üí256 ‚Üí FC 256‚Üí1
        Tanh (output: -1 to +1)
    
    # 3b. Opening Policy Head
    opening_head:
        Conv 128‚Üí64, 1√ó1
        Flatten ‚Üí FC 4096‚Üí1968 (common opening moves)
        
    # 3c. Tactical Policy Head  
    tactical_head:
        Conv 128‚Üí64, 1√ó1
        Flatten ‚Üí FC 4096‚Üí4096 (all legal moves)
```

**Key Features:**
- **No Transformer**: Too slow for 2 vCPU target
- **15 ResBlocks**: Sweet spot between depth and speed
- **Dual Policy Heads**: Opening uses curated move database, tactical uses full search
- **Total Parameters**: ~8-10M (vs 38M in failed version)
- **Expected ONNX Size**: 25-30 MB

---

## üìä Data Pipeline Redesign

### Phase 1: Opening Database v2 üîß

**Problems with Current Opening Architect:**
1. Incorrect perspective scoring
2. No move frequency weighting
3. Missing transposition tables
4. Too low ELO threshold

**New Design:**

```python
# Opening Book Structure
{
    "fen_canonical": {
        "move": {
            "frequency": 1500,  # times played
            "white_perspective_score": 0.65,  # 0-1 scale
            "black_perspective_score": 0.35,
            "avg_elo": 2650,
            "continuations": {
                "next_move": {...}  # recursive
            }
        }
    }
}
```

**Collection Strategy:**

```python
# Configuration
MIN_ELO = 2600  # Top 1% players
MIN_GAMES_PER_POSITION = 50
MAX_DEPTH = 25 moves (was 20)
DATABASE_SOURCE = "lichess_db_standard_rated_2024-04.pgn.zst"  # Recent!

# Perspective-Aware Scoring
def calculate_move_quality(fen, move, result):
    board = chess.Board(fen)
    is_white_turn = board.turn == chess.WHITE
    
    if result == '1-0':
        score = 1.0 if is_white_turn else 0.0
    elif result == '0-1':
        score = 0.0 if is_white_turn else 1.0
    else:
        score = 0.5
    
    return score

# Transposition Handling
def canonical_fen(fen):
    # Normalize pawn structure, castling rights order
    # Hash for quick lookup
    return normalized_fen
```

**Expected Output:**
- 200K-300K unique positions
- 800K-1.2M move evaluations
- Database size: ~150-200 MB SQLite

---

### Phase 2: Match Training Data üéÆ

**Current Issues:**
- 2017 games are outdated
- 2000 ELO includes amateurs
- Only opening moves (first 20 ply)

**New Collection Plan:**

```python
# Target Specifications
DATASET = "lichess_db_standard_rated_2024-10.pgn.zst"  # Latest available
MIN_ELO = 2500
TARGET_GAMES = 2_000_000  # 2 million high-quality games
MAX_PLY_PER_GAME = 80  # Full games

# Game Filtering
def is_quality_game(game):
    # Must have:
    - Both players 2500+
    - Game length 40-200 moves (no quick resignations)
    - Time control: Classical (1800+ seconds)
    - No engine flags
    - Decisive or hard-fought draw
    
# Position Extraction
for each game:
    for each position after move 10:  # Skip obvious opening
        if position_is_interesting():  # Has tactics, imbalance, etc.
            save_training_sample(fen, eval_score, best_move)
```

**Interesting Position Criteria:**
- Material imbalance
- Hanging pieces
- Check/pin/fork potential
- Pawn breaks available
- King safety compromised

**Expected Output:**
- 3-5 million training positions
- Balanced phase distribution (30% opening, 50% middlegame, 20% endgame)
- Database: ~500-800 MB

---

### Phase 3: Enhanced Puzzle Integration üß©

**Current Status:** 500K puzzles collected but not used in training

**Integration Strategy:**

```python
# Puzzle Augmentation
1. Extract Initial Position FEN
2. Extract Solution Move Sequence
3. Generate "Before/After" Training Pairs:
   - Input: Position before tactic
   - Target: Best move + evaluation boost
   
# Puzzle Difficulty Stratification
EASY = rating < 1800    # 20% weight
MEDIUM = 1800-2200      # 30% weight  
HARD = 2200-2600        # 35% weight
MASTER = 2600+          # 15% weight (most valuable)

# Theme Extraction Enhancement
TACTICAL_THEMES = {
    'fork': pattern_matcher_fork(),
    'pin': pattern_matcher_pin(),
    'skewer': pattern_matcher_skewer(),
    'discovered_attack': ...,
    'deflection': ...,
    'remove_defender': ...
}

# For each puzzle:
    if 'fork' in themes or is_detected_fork(position):
        add_explicit_fork_label()
```

**Training Use:**
- 30% of training batches are puzzle positions
- Weighted sampling by difficulty
- Tactical head gets extra supervision here

**Expected Output:**
- 500K puzzle positions (already have)
- 150K explicitly labeled tactical patterns
- Additional synthetic variations: 200K

---

### Phase 4: Endgame Augmentation ‚ôüÔ∏è

**Current Status:** 600K positions (3 workers √ó 200K)

**Enhancement Plan:**

```python
# Current Limitations
- Only 3-4-5 piece endgames
- Only WDL labels (Win/Draw/Loss)
- Some critical scenarios under-represented

# Expansion Strategy

# Option A: Add More Workers (Easy)
WORKERS = 6  # Double the workforce
POSITIONS_PER_WORKER = 200K
TOTAL = 1.2M positions

# Option B: Add DTZ Labels (Better)
for position in existing_db:
    if tablebase.has_dtz():
        dtz = tablebase.probe_dtz(position)
        # DTZ tells "moves until 50-move rule" or "mate"
        update_label(position, wdl, dtz)

# Option C: Critical Scenario Oversampling
SCENARIOS_WEIGHTED = {
    'KQvKR': 2.0,   # Complex queen endings
    'KRvKR': 3.0,   # Most common
    'KPvK': 5.0,    # Fundamental
    'KRPvKR': 2.0,  # Crucial technique
}

# Option D: Add 6-Piece (If time permits)
# Note: 6-piece tablebases are HUGE (1.2TB total)
# Only download most common: KQRvKR, KRRvKR, KRBvKR
```

**Recommendation:**
- Run 3 more workers (600K ‚Üí 1.2M positions) ‚úÖ **Priority 1**
- Add DTZ labels to existing data ‚úÖ **Priority 2**  
- Skip 6-piece (diminishing returns, storage cost)

**Time Required:**
- 3 workers √ó 12 hours = 36 compute-hours (parallelize = 12 real hours)
- DTZ labeling: 2-3 hours for 600K positions

---

## üîÑ Complete Training Pipeline

### Stage 1: Foundation Training (Supervised)

```python
# Data Composition
opening_positions: 30%    # From Opening DB v2
midgame_positions: 40%    # From filtered match games
puzzle_positions: 20%     # Tactical training
endgame_positions: 10%    # Tablebase positions

# Batch Construction
BATCH_SIZE = 512
ACCUMULATION_STEPS = 2
EFFECTIVE_BATCH = 1024

# Loss Function (Multi-Task)
loss_value = MSE(predicted_value, true_value)
loss_opening_policy = CrossEntropy(pred_opening, best_opening_move)
loss_tactical_policy = CrossEntropy(pred_tactical, best_tactical_move)

total_loss = loss_value + 0.5 * loss_opening_policy + 0.5 * loss_tactical_policy

# Training Schedule
EPOCHS = 10
LEARNING_RATE = 1e-4 ‚Üí 1e-6 (OneCycleLR)
WEIGHT_DECAY = 1e-4
GRADIENT_CLIP = 1.0

# Hardware
Google Colab T4 GPU (free)
Mixed Precision (FP16)
Expected Time: 24-30 hours
```

### Stage 2: Self-Play Fine-Tuning (Reinforcement)

**This is the continuous improvement loop**

```python
# Self-Play Infrastructure

# HF Space Configuration
SPACE_NAME = "GambitFlow/Synapse-SelfPlay-Worker"
HARDWARE = "cpu-basic" (2 vCPU, 16GB RAM)
SECRETS:
    HF_TOKEN = <write access>
    WORKER_ID = 1  # Set per clone

# Game Generation
GAMES_PER_BATCH = 100
TIME_PER_MOVE = 1.0s
SEARCH_DEPTH = model_inference() + minimax(depth=2)

# Match Protocol
def self_play_match():
    white_model = "Synapse-Base"
    black_model = "Synapse-Base"
    
    game = chess.pgn.Game()
    board = chess.Board()
    
    while not board.is_game_over():
        if board.turn == chess.WHITE:
            move = white_model.select_move(board)
        else:
            move = black_model.select_move(board)
        
        board.push(move)
        game.add_main_variation(move)
    
    return game  # PGN format

# Data Upload
if len(games) >= GAMES_PER_BATCH:
    pgn_file = f"worker_{WORKER_ID}_batch_{batch_num}.pgn"
    upload_to_hf(
        repo=f"GambitFlow/SelfPlay-Data",
        path=f"Worker_{WORKER_ID}/{pgn_file}",
        data=games_to_pgn(games)
    )
    games.clear()
```

### Stage 3: Evaluation & Model Update

**Automated Tournament System**

```python
# Evaluation Notebook (Run periodically)

# 1. Collect Self-Play Data
def gather_training_data():
    all_games = []
    for worker_id in range(1, N_WORKERS+1):
        folder = f"Worker_{worker_id}/"
        files = list_files_in_hf_dataset(
            repo="GambitFlow/SelfPlay-Data",
            folder=folder
        )
        for file in files:
            games = load_pgn(file)
            all_games.extend(games)
    
    return all_games  # Format: List[chess.pgn.Game]

# 2. Convert to Training Format
def games_to_training_data(games):
    positions = []
    for game in games:
        result = game_result_to_value(game.headers["Result"])
        board = game.board()
        
        for move in game.mainline_moves():
            fen = board.fen()
            move_uci = move.uci()
            
            # Extract features (28 channels)
            features = fen_to_28_channel_tensor(fen)
            
            # Training sample
            positions.append({
                'input': features,
                'value_target': result,
                'policy_target': move_uci
            })
            
            board.push(move)
    
    return positions

# 3. Fine-Tune Model
def fine_tune_model(base_model_path, new_data):
    # Load existing weights
    model = SynapseBase()
    model.load_state_dict(torch.load(base_model_path))
    
    # Fine-tune with lower learning rate
    optimizer = AdamW(model.parameters(), lr=1e-5)
    
    train(model, new_data, epochs=3)
    
    # Save as v2
    torch.save(model.state_dict(), "synapse_base_v2.pth")
    export_to_onnx(model, "synapse_base_v2.onnx")
    
    return model

# 4. Evaluation Match
def tournament_match(model_a, model_b, games=100):
    wins_a = 0
    wins_b = 0
    draws = 0
    
    for game_num in range(games):
        # Alternate colors
        if game_num % 2 == 0:
            white, black = model_a, model_b
        else:
            white, black = model_b, model_a
        
        result = play_game(white, black)
        
        if result == '1-0':
            if game_num % 2 == 0: wins_a += 1
            else: wins_b += 1
        elif result == '0-1':
            if game_num % 2 == 0: wins_b += 1
            else: wins_a += 1
        else:
            draws += 1
    
    return wins_a, wins_b, draws

# 5. Model Promotion Logic
base_v1 = load_model("Synapse-Base")
base_v2 = load_model("Synapse-Base-v2")

wins_v1, wins_v2, draws = tournament_match(base_v1, base_v2, games=100)

if wins_v2 > wins_v1:
    print("üéâ V2 is superior! Promoting...")
    
    # Delete old model
    delete_hf_model("GambitFlow/Synapse-Base")
    
    # Rename v2 to base
    upload_model(base_v2, "GambitFlow/Synapse-Base")
    
    # Clear training data for that worker
    worker_folder = select_random_worker()  # Balance data distribution
    delete_hf_folder(f"GambitFlow/SelfPlay-Data/Worker_{worker_folder}")
    
    print(f"‚úÖ Worker_{worker_folder} data cleared for fresh games")
else:
    print(f"V1 still stronger: {wins_v1}-{wins_v2}-{draws}")
```

---

## üöÄ Implementation Sequence

### Week 1: Data Preparation

#### Day 1-2: Opening Database v2
```markdown
**File:** `Opening_Architect_v2.ipynb`

1. Download lichess_db_standard_rated_2024-10.pgn.zst
2. Implement perspective-aware scoring
3. Add transposition detection
4. Filter: MIN_ELO=2600, MIN_FREQUENCY=50
5. Build recursive continuation tree (depth=25)
6. Export to SQLite: `opening_theory_v2.db`
7. Upload to HF: `GambitFlow/Elite-Data/opening_theory_v2.db`

**Verification:**
- Sample 100 positions
- Check scoring makes sense for both colors
- Verify move frequencies match expected popularity

**Est. Time:** 15-20 hours compute
```

#### Day 3-4: Match Training Data
```markdown
**File:** `Match_Data_Curator.ipynb`

1. Download lichess_db_2024-10.pgn.zst
2. Filter: MIN_ELO=2500, classical time control
3. Extract interesting positions:
   - Skip first 10 moves (covered by opening DB)
   - Identify tactical moments (material imbalance, threats)
   - Sample middlegame/endgame proportionally
4. Label with stockfish evaluation (depth 20)
5. Export to SQLite: `match_positions_v2.db` (3-5M positions)
6. Upload to HF: `GambitFlow/Elite-Data/match_positions_v2.db`

**Verification:**
- Check phase distribution (30/50/20)
- Verify eval quality (compare SF vs actual game outcome)

**Est. Time:** 30-40 hours compute (can parallelize)
```

#### Day 5: Puzzle Integration
```markdown
**File:** `Tactical_Forge_v2.ipynb`

1. Load existing puzzle DB (500K puzzles)
2. Stratify by difficulty (easy/medium/hard/master)
3. Add explicit tactical pattern labels:
   ```python
   def detect_fork(board, move):
       # After move, count pieces attacked by moving piece
       ...
   ```
4. Generate synthetic variations (200K more)
5. Export: `tactical_positions_v2.db`
6. Upload to HF

**Est. Time:** 5-6 hours
```

#### Day 6-7: Endgame Enhancement
```markdown
**File:** `Endgame_Oracle_v2.ipynb`

**Task 1:** Generate more positions
1. Set WORKER_ID = 4, 5, 6
2. Run 3 parallel sessions
3. Each generates 200K positions (total: 600K new + 600K old = 1.2M)

**Task 2:** Add DTZ labels
1. For all 1.2M positions:
   ```python
   if tablebase.has_dtz(board):
       dtz = tablebase.probe_dtz(board)
       update_db(fen, wdl, dtz)
   ```

**Est. Time:** 12 hours (parallel) + 3 hours (DTZ)

**Upload:** `endgame_positions_v2.db` to HF
```

### Week 2: Model Development

#### Day 8-9: Architecture Implementation
```markdown
**File:** `Synapse_Base_Architecture.ipynb`

1. Define 28-channel input tensor:
   ```python
   def fen_to_28_channels(fen):
       # 12 pieces + 8 context + 8 history
       ...
   ```

2. Implement model:
   ```python
   class SynapseBase(nn.Module):
       def __init__(self):
           self.conv_stem = nn.Conv2d(28, 128, 3, padding=1)
           self.res_tower = nn.Sequential(*[ResBlock(128) for _ in range(15)])
           self.value_head = ...
           self.opening_head = ...
           self.tactical_head = ...
   ```

3. Test forward pass:
   ```python
   dummy = torch.randn(1, 28, 8, 8)
   value, opening_logits, tactical_logits = model(dummy)
   assert value.shape == (1, 1)
   assert opening_logits.shape == (1, 1968)
   assert tactical_logits.shape == (1, 4096)
   ```

**Est. Time:** 6-8 hours
```

#### Day 10-12: Training Pipeline
```markdown
**File:** `Synapse_Base_Training.ipynb`

1. Create unified dataset loader:
   ```python
   class ChessDataset(IterableDataset):
       def __init__(self, db_paths, sampling_weights):
           self.sources = {
               'opening': (db_paths['opening'], 0.30),
               'midgame': (db_paths['midgame'], 0.40),
               'tactical': (db_paths['tactical'], 0.20),
               'endgame': (db_paths['endgame'], 0.10)
           }
       
       def __iter__(self):
           # Weighted sampling from all sources
           ...
   ```

2. Multi-task loss:
   ```python
   def compute_loss(pred, target):
       loss_val = F.mse_loss(pred['value'], target['value'])
       loss_open = F.cross_entropy(pred['opening'], target['opening_move'])
       loss_tact = F.cross_entropy(pred['tactical'], target['tactical_move'])
       return loss_val + 0.5*loss_open + 0.5*loss_tact
   ```

3. Training loop:
   - Epochs: 10
   - Batch size: 512
   - Accumulation: 2
   - Mixed precision FP16
   - OneCycleLR scheduler

4. Save checkpoints every epoch

**Est. Time:** 24-30 hours training on T4 GPU
```

#### Day 13: ONNX Export & Validation
```markdown
**File:** `Synapse_Export.ipynb`

1. Load best checkpoint
2. Export to ONNX:
   ```python
   torch.onnx.export(
       model,
       dummy_input,
       "synapse_base.onnx",
       opset_version=17,
       input_names=['board'],
       output_names=['value', 'opening_policy', 'tactical_policy']
   )
   ```

3. Validate ONNX inference:
   ```python
   import onnxruntime as ort
   session = ort.InferenceSession("synapse_base.onnx")
   output = session.run(None, {'board': test_position})
   ```

4. Upload both files:
   - `synapse_base.pth` (PyTorch weights for future fine-tuning)
   - `synapse_base.onnx` (Inference model)
   
   To: `GambitFlow/Synapse-Base`

**Verification:**
- ONNX size: 20-30 MB ‚úÖ
- Inference time on CPU: <100ms ‚úÖ
- Sanity check: Plays legal moves ‚úÖ

**Est. Time:** 2-3 hours
```

### Week 3: Self-Play Infrastructure

#### Day 14-15: HF Spaces Self-Play Worker
```markdown
**File:** `spaces/synapse-selfplay-worker/app.py`

```python
import chess
import chess.pgn
import onnxruntime as ort
import numpy as np
from huggingface_hub import HfApi, hf_hub_download
import os
import time

# Configuration
WORKER_ID = int(os.environ.get('WORKER_ID', 0))
HF_TOKEN = os.environ['HF_TOKEN']
MODEL_REPO = "GambitFlow/Synapse-Base"
DATA_REPO = "GambitFlow/SelfPlay-Data"
GAMES_PER_BATCH = 100

# Load model
model_path = hf_hub_download(repo_id=MODEL_REPO, filename="synapse_base.onnx")
session = ort.InferenceSession(model_path)

# Search algorithm
def select_move(board, session):
    # 1. Get model evaluation
    features = board_to_28_channels(board)
    value, opening_policy, tactical_policy = session.run(None, {'board': features})
    
    # 2. Opening book lookup (if in opening phase)
    if board.fullmove_number <= 15:
        book_move = query_opening_db(board.fen())
        if book_move and random.random() < 0.8:  # 80% follow book
            return book_move
    
    # 3. Minimax search (depth 2-3)
    legal_moves = list(board.legal_moves)
    best_move = None
    best_eval = -999
    
    for move in legal_moves:
        board.push(move)
        
        # Evaluate position after move
        eval_score = -get_model_eval(board, session)
        
        # If endgame, check tablebase
        if count_pieces(board) <= 5:
            tb_result = query_tablebase(board)
            if tb_result is not None:
                eval_score = tb_result
        
        board.pop()
        
        if eval_score > best_eval:
            best_eval = eval_score
            best_move = move
    
    return best_move

# Main loop
def main():
    if WORKER_ID == 0:
        print("WORKER_ID not set. Exiting.")
        return
    
    api = HfApi(token=HF_TOKEN)
    games_buffer = []
    batch_num = 0
    
    print(f"Worker {WORKER_ID} starting...")
    
    while True:
        # Play one game
        game = chess.pgn.Game()
        board = chess.Board()
        
        move_count = 0
        while not board.is_game_over() and move_count < 200:
            move = select_move(board, session)
            board.push(move)
            game.add_main_variation(move)
            move_count += 1
        
        # Record result
        result = board.result()
        game.headers["Result"] = result
        game.headers["White"] = "Synapse-Base"
       ```python
        game.headers["Black"] = "Synapse-Base"
        game.headers["Event"] = f"Self-Play Worker {WORKER_ID}"
        game.headers["Date"] = time.strftime("%Y.%m.%d")
        
        games_buffer.append(game)
        print(f"Game {len(games_buffer)}/{GAMES_PER_BATCH} completed: {result}")
        
        # Upload batch when ready
        if len(games_buffer) >= GAMES_PER_BATCH:
            batch_num += 1
            pgn_string = ""
            
            for g in games_buffer:
                pgn_string += str(g) + "\n\n"
            
            filename = f"worker_{WORKER_ID}_batch_{batch_num}.pgn"
            filepath = f"/tmp/{filename}"
            
            with open(filepath, 'w') as f:
                f.write(pgn_string)
            
            # Upload to HF
            try:
                api.upload_file(
                    path_or_fileobj=filepath,
                    path_in_repo=f"Worker_{WORKER_ID}/{filename}",
                    repo_id=DATA_REPO,
                    repo_type="dataset",
                    commit_message=f"Worker {WORKER_ID} - Batch {batch_num}"
                )
                print(f"‚úÖ Uploaded batch {batch_num} ({GAMES_PER_BATCH} games)")
                games_buffer.clear()
                
            except Exception as e:
                print(f"‚ùå Upload failed: {e}")
                time.sleep(60)  # Wait before retry
        
        time.sleep(1)  # Prevent rate limiting

if __name__ == "__main__":
    main()
```

**Files to create:**
1. `app.py` (above)
2. `requirements.txt`:
   ```
   chess==1.11.2
   onnxruntime==1.19.2
   huggingface_hub==0.36.0
   numpy==1.26.0
   ```
3. `README.md`:
   ```markdown
   # Synapse Self-Play Worker
   
   ## Configuration
   Set these secrets in Space settings:
   - HF_TOKEN: Your write-access token
   - WORKER_ID: Unique integer (1, 2, 3, ...)
   
   ## Cloning
   1. Clone this space
   2. Set WORKER_ID to next available number
   3. Keep HF_TOKEN the same
   4. Space will auto-start generating games
   ```

**Est. Time:** 8-10 hours development + testing
```

#### Day 16: Helper Functions & Utilities
```markdown
**Create utility modules for the Space:**

**File:** `utils/board_encoding.py`
```python
import numpy as np
import chess

def board_to_28_channels(board):
    """Convert chess.Board to 28-channel tensor"""
    tensor = np.zeros((28, 8, 8), dtype=np.float32)
    
    # Channels 0-11: Pieces
    piece_map = {
        chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,
        chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5
    }
    
    for square in chess.SQUARES:
        piece = board.piece_at(square)
        if piece:
            channel = piece_map[piece.piece_type]
            if piece.color == chess.BLACK:
                channel += 6
            rank, file = divmod(square, 8)
            tensor[channel, 7-rank, file] = 1.0
    
    # Channel 12: Turn
    if board.turn == chess.WHITE:
        tensor[12, :, :] = 1.0
    
    # Channels 13-16: Castling rights
    if board.has_kingside_castling_rights(chess.WHITE):
        tensor[13, :, :] = 1.0
    if board.has_queenside_castling_rights(chess.WHITE):
        tensor[14, :, :] = 1.0
    if board.has_kingside_castling_rights(chess.BLACK):
        tensor[15, :, :] = 1.0
    if board.has_queenside_castling_rights(chess.BLACK):
        tensor[16, :, :] = 1.0
    
    # Channels 17-19: En passant file (if exists)
    if board.ep_square:
        ep_file = chess.square_file(board.ep_square)
        tensor[17 + ep_file % 3, :, :] = 1.0  # Simplified encoding
    
    # Channels 20-27: Move history (last 4 moves)
    # This requires tracking in game loop - placeholder for now
    
    return tensor.astype(np.float32)
```

**File:** `utils/opening_book.py`
```python
import sqlite3
import chess
from huggingface_hub import hf_hub_download

class OpeningBook:
    def __init__(self):
        db_path = hf_hub_download(
            repo_id="GambitFlow/Elite-Data",
            filename="opening_theory_v2.db",
            repo_type="dataset"
        )
        self.conn = sqlite3.connect(db_path)
        self.cursor = self.conn.cursor()
    
    def query(self, fen):
        """Get best move from opening book"""
        # Normalize FEN (remove move counters)
        clean_fen = " ".join(fen.split()[:4])
        
        self.cursor.execute(
            "SELECT move_data FROM positions WHERE fen = ?",
            (clean_fen,)
        )
        result = self.cursor.fetchone()
        
        if not result:
            return None
        
        import json
        moves = json.loads(result[0])
        
        # Select move with highest frequency and score
        best_move = None
        best_score = -999
        
        for move_san, data in moves.items():
            score = data['frequency'] * data['score']
            if score > best_score:
                best_score = score
                best_move = move_san
        
        return best_move
```

**File:** `utils/endgame_tablebase.py`
```python
import chess
import chess.syzygy

class EndgameOracle:
    def __init__(self):
        # Download 3-4-5 piece tablebases (pre-packaged)
        # For production, these would be baked into Docker image
        self.tablebase = None  # Initialize lazily
    
    def query(self, board):
        """Query Syzygy tablebase for endgame positions"""
        if len(board.piece_map()) > 5:
            return None  # Not an endgame
        
        if self.tablebase is None:
            # Lazy load (expensive operation)
            import os
            tb_path = os.environ.get('SYZYGY_PATH', '/app/tablebases')
            if os.path.exists(tb_path):
                self.tablebase = chess.syzygy.open_tablebase(tb_path)
            else:
                return None
        
        try:
            wdl = self.tablebase.probe_wdl(board)
            # Convert to evaluation score
            if wdl > 0:
                return 999  # Winning
            elif wdl < 0:
                return -999  # Losing
            else:
                return 0  # Draw
        except:
            return None
```

**Est. Time:** 4-6 hours
```

### Week 4: Fine-Tuning & Evaluation System

#### Day 17-18: Fine-Tuning Pipeline
```markdown
**File:** `Synapse_FineTune.ipynb`

Purpose: Periodically fine-tune base model with self-play data

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from huggingface_hub import HfApi, hf_hub_download
import chess.pgn
import io

# === STEP 1: Download Self-Play Games ===

def download_selfplay_data(worker_id=None):
    """Download PGN files from HF dataset"""
    api = HfApi()
    
    files = api.list_repo_files(
        repo_id="GambitFlow/SelfPlay-Data",
        repo_type="dataset"
    )
    
    pgn_files = [f for f in files if f.endswith('.pgn')]
    
    if worker_id:
        pgn_files = [f for f in pgn_files if f.startswith(f"Worker_{worker_id}/")]
    
    all_games = []
    
    for file_path in pgn_files:
        print(f"Downloading {file_path}...")
        local_path = hf_hub_download(
            repo_id="GambitFlow/SelfPlay-Data",
            filename=file_path,
            repo_type="dataset"
        )
        
        with open(local_path) as pgn:
            while True:
                game = chess.pgn.read_game(pgn)
                if game is None:
                    break
                all_games.append(game)
    
    print(f"Total games collected: {len(all_games)}")
    return all_games

# === STEP 2: Convert to Training Format ===

def game_to_training_samples(game):
    """Extract training positions from a game"""
    samples = []
    
    result_map = {'1-0': 1.0, '0-1': -1.0, '1/2-1/2': 0.0}
    result_value = result_map.get(game.headers.get('Result', '1/2-1/2'), 0.0)
    
    board = game.board()
    move_history = []
    
    for move_num, move in enumerate(game.mainline_moves()):
        # Skip very early opening (covered by opening DB)
        if move_num < 10:
            board.push(move)
            move_history.append(move)
            continue
        
        # Create training sample
        fen = board.fen()
        features = fen_to_28_channels_with_history(board, move_history[-8:])
        
        # Determine value target (discounted by game progress)
        discount = 1.0 - (move_num / 200.0)  # Later moves matter less
        value_target = result_value * discount
        
        # Policy target (the move actually played)
        policy_target = move.uci()
        
        samples.append({
            'features': features,
            'value': value_target,
            'move': policy_target,
            'phase': 'midgame' if move_num < 40 else 'endgame'
        })
        
        board.push(move)
        move_history.append(move)
    
    return samples

# === STEP 3: Fine-Tuning Dataset ===

class SelfPlayDataset(Dataset):
    def __init__(self, games):
        self.samples = []
        
        print("Converting games to training samples...")
        for game in games:
            self.samples.extend(game_to_training_samples(game))
        
        print(f"Total training samples: {len(self.samples)}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.tensor(sample['features'], dtype=torch.float32),
            torch.tensor([sample['value']], dtype=torch.float32),
            sample['move']  # Will be encoded to index
        )

# === STEP 4: Fine-Tuning Loop ===

def fine_tune_model(base_model_path, games, version_suffix):
    """Fine-tune existing model with new games"""
    
    # Load base model
    model = SynapseBase()
    state_dict = torch.load(base_model_path, map_location='cpu')
    model.load_state_dict(state_dict)
    model = model.to(device)
    
    # Prepare data
    dataset = SelfPlayDataset(games)
    loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)
    
    # Fine-tuning optimizer (lower learning rate)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-5,  # 10x lower than initial training
        weight_decay=1e-4
    )
    
    criterion_value = nn.MSELoss()
    criterion_policy = nn.CrossEntropyLoss()
    
    EPOCHS = 3  # Short fine-tuning
    
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        
        for batch_idx, (features, values, moves) in enumerate(loader):
            features = features.to(device)
            values = values.to(device)
            
            # Forward pass
            pred_value, pred_opening, pred_tactical = model(features)
            
            # Loss computation
            loss_value = criterion_value(pred_value, values)
            
            # Policy loss (simplified - using tactical head)
            move_indices = moves_to_indices(moves)  # Convert UCI to indices
            loss_policy = criterion_policy(pred_tactical, move_indices.to(device))
            
            loss = loss_value + 0.3 * loss_policy  # Lower policy weight in fine-tuning
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch+1}/{EPOCHS} | Batch {batch_idx} | Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(loader)
        print(f"Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}")
    
    # Save fine-tuned model
    output_path = f"synapse_base_{version_suffix}.pth"
    torch.save(model.state_dict(), output_path)
    
    # Export to ONNX
    model.eval()
    model.to('cpu')
    dummy_input = torch.randn(1, 28, 8, 8)
    
    onnx_path = f"synapse_base_{version_suffix}.onnx"
    torch.onnx.export(
        model, dummy_input, onnx_path,
        opset_version=17,
        input_names=['board'],
        output_names=['value', 'opening_policy', 'tactical_policy']
    )
    
    print(f"‚úÖ Fine-tuned model saved: {output_path}, {onnx_path}")
    return output_path, onnx_path

# === MAIN EXECUTION ===

if __name__ == "__main__":
    # Configuration
    WORKER_TO_USE = 1  # Which worker's data to use
    VERSION = "v2"
    
    # Step 1: Download games
    games = download_selfplay_data(worker_id=WORKER_TO_USE)
    
    if len(games) < 100:
        print("‚ö†Ô∏è Not enough games for fine-tuning. Need at least 100.")
        exit()
    
    # Step 2: Download base model
    base_pth = hf_hub_download(
        repo_id="GambitFlow/Synapse-Base",
        filename="synapse_base.pth"
    )
    
    # Step 3: Fine-tune
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pth_path, onnx_path = fine_tune_model(base_pth, games, VERSION)
    
    print(f"\nüéâ Fine-tuning complete! Now run evaluation...")
```

**Est. Time:** 3-5 hours per fine-tuning cycle
```

#### Day 19: Evaluation Tournament System
```markdown
**File:** `Synapse_Evaluation.ipynb`

Purpose: Play 100-game match between base and v2, promote winner

```python
import chess
import chess.pgn
import onnxruntime as ort
import numpy as np
from huggingface_hub import HfApi, hf_hub_download
import time

# === MODEL WRAPPER ===

class ChessEngine:
    def __init__(self, onnx_path, name):
        self.session = ort.InferenceSession(onnx_path)
        self.name = name
        self.opening_book = OpeningBook()  # From utils
        self.endgame_tb = EndgameOracle()
    
    def select_move(self, board):
        """Select best move using model + search"""
        
        # 1. Opening book (first 15 moves)
        if board.fullmove_number <= 15:
            book_move = self.opening_book.query(board.fen())
            if book_move:
                try:
                    return board.parse_san(book_move)
                except:
                    pass  # Invalid book move
        
        # 2. Endgame tablebase (5 pieces or less)
        if len(board.piece_map()) <= 5:
            tb_move = self.endgame_tb.get_best_move(board)
            if tb_move:
                return tb_move
        
        # 3. Model evaluation + minimax
        legal_moves = list(board.legal_moves)
        
        if not legal_moves:
            return None
        
        best_move = None
        best_score = -9999
        
        for move in legal_moves:
            board.push(move)
            
            # Evaluate position
            features = board_to_28_channels(board).reshape(1, 28, 8, 8)
            outputs = self.session.run(None, {'board': features})
            eval_score = -float(outputs[0][0][0])  # Negate for opponent
            
            board.pop()
            
            if eval_score > best_score:
                best_score = eval_score
                best_move = move
        
        return best_move

# === GAME ENGINE ===

def play_game(white_engine, black_engine, time_limit=1.0):
    """Play a single game between two engines"""
    board = chess.Board()
    game = chess.pgn.Game()
    node = game
    
    move_count = 0
    max_moves = 200
    
    while not board.is_game_over() and move_count < max_moves:
        # Select move
        if board.turn == chess.WHITE:
            move = white_engine.select_move(board)
        else:
            move = black_engine.select_move(board)
        
        if move is None:
            break
        
        # Make move
        board.push(move)
        node = node.add_variation(move)
        move_count += 1
        
        # Time limit (optional)
        time.sleep(0.1)
    
    # Determine result
    result = board.result()
    game.headers["Result"] = result
    game.headers["White"] = white_engine.name
    game.headers["Black"] = black_engine.name
    
    return result, game

# === TOURNAMENT ===

def tournament(engine_a, engine_b, num_games=100):
    """Play a match between two engines"""
    
    results = {'1-0': 0, '0-1': 0, '1/2-1/2': 0}
    games = []
    
    print(f"üèÜ Starting tournament: {engine_a.name} vs {engine_b.name}")
    print(f"Games: {num_games}")
    print("-" * 50)
    
    for game_num in range(num_games):
        # Alternate colors
        if game_num % 2 == 0:
            white, black = engine_a, engine_b
        else:
            white, black = engine_b, engine_a
        
        result, game = play_game(white, black)
        results[result] += 1
        games.append(game)
        
        # Track wins per engine
        if result == '1-0':
            winner = white.name
        elif result == '0-1':
            winner = black.name
        else:
            winner = "Draw"
        
        print(f"Game {game_num+1}/{num_games}: {winner} | Score: {results}")
    
    # Calculate final score from engine_a's perspective
    score_a = 0
    for game_num, result in enumerate([g.headers['Result'] for g in games]):
        if game_num % 2 == 0:  # engine_a was white
            if result == '1-0': score_a += 1
            elif result == '1/2-1/2': score_a += 0.5
        else:  # engine_a was black
            if result == '0-1': score_a += 1
            elif result == '1/2-1/2': score_a += 0.5
    
    score_b = num_games - score_a
    
    print("\n" + "=" * 50)
    print(f"FINAL SCORE: {engine_a.name} {score_a} - {score_b} {engine_b.name}")
    print("=" * 50)
    
    return score_a, score_b, games

# === MODEL PROMOTION ===

def promote_model_if_better():
    """Main evaluation and promotion logic"""
    
    api = HfApi(token=HF_TOKEN)
    
    # Download both models
    print("üì• Downloading models...")
    base_path = hf_hub_download(
        repo_id="GambitFlow/Synapse-Base",
        filename="synapse_base.onnx"
    )
    
    v2_path = hf_hub_download(
        repo_id="GambitFlow/Synapse-Base",
        filename="synapse_base_v2.onnx"
    )
    
    # Create engines
    engine_base = ChessEngine(base_path, "Synapse-Base")
    engine_v2 = ChessEngine(v2_path, "Synapse-Base-v2")
    
    # Run tournament
    score_base, score_v2, games = tournament(engine_base, engine_v2, num_games=100)
    
    # Decision
    if score_v2 > score_base:
        print("\nüéâ V2 IS SUPERIOR! Promoting to main model...")
        
        # Upload v2 as new base
        api.upload_file(
            path_or_fileobj="synapse_base_v2.pth",
            path_in_repo="synapse_base.pth",
            repo_id="GambitFlow/Synapse-Base",
            commit_message=f"Promotion: v2 defeated v1 ({score_v2}-{score_base})"
        )
        
        api.upload_file(
            path_or_fileobj="synapse_base_v2.onnx",
            path_in_repo="synapse_base.onnx",
            repo_id="GambitFlow/Synapse-Base",
            commit_message=f"Promotion: v2 defeated v1 ({score_v2}-{score_base})"
        )
        
        # Clear one worker's data folder for fresh games
        worker_to_clear = 1  # Could rotate
        files = api.list_repo_files(
            repo_id="GambitFlow/SelfPlay-Data",
            repo_type="dataset"
        )
        
        files_to_delete = [f for f in files if f.startswith(f"Worker_{worker_to_clear}/")]
        
        for file in files_to_delete:
            api.delete_file(
                path_in_repo=file,
                repo_id="GambitFlow/SelfPlay-Data",
                repo_type="dataset"
            )
        
        print(f"‚úÖ Worker_{worker_to_clear} data cleared")
        print("‚úÖ Model promotion complete!")
        
    else:
        print(f"\nüìä Base model remains superior: {score_base}-{score_v2}")
        print("V2 will not be promoted.")
    
    # Save tournament games for analysis
    with open("evaluation_games.pgn", "w") as f:
        for game in games:
            f.write(str(game) + "\n\n")
    
    print("\nüìù Evaluation games saved to evaluation_games.pgn")

# === EXECUTION ===
if __name__ == "__main__":
    HF_TOKEN = "your_token_here"  # Set this
    promote_model_if_better()
```

**Est. Time:** 10-15 hours (100 games at ~5-10 min each)
```

#### Day 20-21: Documentation & Testing
```markdown
**Tasks:**

1. **Create comprehensive README.md** for each component:
   - Main project README
   - Data collection notebooks
   - Training notebooks
   - Self-play space
   - Fine-tuning pipeline

2. **Test full pipeline end-to-end:**
   ```bash
   # Checklist:
   ‚úÖ Can download all datasets from HF
   ‚úÖ Model trains without errors
   ‚úÖ ONNX export works
   ‚úÖ Inference runs on CPU (2 vCPU)
   ‚úÖ Self-play space generates games
   ‚úÖ Fine-tuning improves model
   ‚úÖ Evaluation tournament runs
   ‚úÖ Model promotion works
   ```

3. **Performance benchmarks:**
   - Inference latency: Target <100ms on 2 vCPU
   - Opening strength: Test against common lines
   - Tactical solving: Test on puzzle set (not used in training)
   - Endgame accuracy: Verify tablebase integration

4. **Create example API endpoint** (for external integration):
   ```python
   # flask_api.py
   from flask import Flask, request, jsonify
   import chess
   import onnxruntime as ort
   
   app = Flask(__name__)
   session = ort.InferenceSession("synapse_base.onnx")
   
   @app.route('/move', methods=['POST'])
   def get_move():
       data = request.json
       fen = data['fen']
       
       board = chess.Board(fen)
       move = select_best_move(board, session)
       
       return jsonify({
           'move': move.uci(),
           'san': board.san(move)
       })
   
   if __name__ == '__main__':
       app.run(host='0.0.0.0', port=7860)
   ```

**Est. Time:** 12-16 hours
```

---

## üìÅ Final Project Structure

```
GambitFlow-Synapse-Base/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_Opening_Architect_v2.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Match_Data_Curator.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Tactical_Forge_v2.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_Endgame_Oracle_v2.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 05_Synapse_Architecture.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 06_Synapse_Training.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 07_Synapse_Export.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 08_Synapse_FineTune.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 09_Synapse_Evaluation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ üìÇ spaces/
‚îÇ   ‚îî‚îÄ‚îÄ synapse-selfplay-worker/
‚îÇ       ‚îú‚îÄ‚îÄ app.py
‚îÇ       ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îú‚îÄ‚îÄ board_encoding.py
‚îÇ           ‚îú‚îÄ‚îÄ opening_book.py
‚îÇ           ‚îî‚îÄ‚îÄ endgame_tablebase.py
‚îÇ
‚îú‚îÄ‚îÄ üìÇ models/
‚îÇ   ‚îú‚îÄ‚îÄ synapse_base.py (architecture definition)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py (training helpers)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ DATA_PIPELINE.md
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ DEPLOYMENT.md
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ Success Criteria

### Minimum Viable Product (MVP)
- ‚úÖ Model size: 20-30 MB ONNX
- ‚úÖ Inference: <100ms on 2 vCPU, 16GB RAM
- ‚úÖ Legal move rate: 100%
- ‚úÖ Opening book coverage: Top 50 openings
- ‚úÖ Endgame accuracy: 95%+ with tablebase
- ‚úÖ Tactical puzzle rating: 2000+

### Target Performance
- üéØ Model size: 25 MB
- üéØ Inference: 50-80ms
- üéØ Playing strength: 2200-2400 ELO (Lichess)
- üéØ Puzzle rating: 2200+
- üéØ Self-play improvement: +50 ELO per generation

### World-Class Goals (Long-term)
- üöÄ Playing strength: 2600+ ELO
- üöÄ Opening novelty generation
- üöÄ Complex tactical combinations (depth 5+)
- üöÄ Strategic understanding (positional play)

---

## ‚ö†Ô∏è Critical Warnings & Mitigations

### 1. Context Window Management
**Risk:** Plan.md is extremely long, might exceed context limits

**Mitigation:**
- This plan is split into logical sections
- Each notebook is self-contained
- Use checkpointing extensively
- Document everything in code comments

### 2. HF Storage Limits
**Risk:** Self-play data accumulates quickly (100 games √ó 50KB = 5MB per batch)

**Mitigation:**
- Delete worker folders after fine-tuning
- Rotate workers (1‚Üí2‚Üí3‚Üí1...)
- Compress PGN files (gzip)
- Set max storage limit (e.g., 500MB total)

### 3. Model Overfitting on Self-Play
**Risk:** Model starts playing predictably against itself

**Mitigation:**
- Mix self-play data with original supervised data (70/30 ratio)
- Add random exploration moves (Œµ=0.1)
- Periodic evaluation against external engines
- Reset to base model if performance degrades

### 4. Inference Speed on CPU
**Risk:** 2 vCPU might be too slow

**Mitigation:**
- Optimize ONNX model (quantization, graph optimization)
- Reduce search depth in critical paths
- Cache position evaluations
- Consider INT8 quantization (2-4x speedup)

### 5. Opening Book Quality
**Risk:** Statistical database might have theoretical errors

**Mitigation:**
- Cross-validate with engine analysis (Stockfish depth 25)
- Require minimum game frequency (50+ games)
- Use recent games only (2024)
- Peer review by strong players

---

## üîß Advanced Optimizations (Optional)

### 1. ONNX Quantization
```python
# After exporting to ONNX
from onnxruntime.quantization import quantize_dynamic

quantize_dynamic(
    model_input="synapse_base.onnx",
    model_output="synapse_base_int8.onnx",
    weight_type=QuantType.QUInt8
)

# Expected: 2-4x faster inference, 25MB ‚Üí 7MB
```




### 2. Pruning & Model Compression
```python
# Prune less important connections
import torch
from torch.nn.utils import prune

def compress_model(model):
    for module in model.modules():
        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=0.2)
            prune.remove(module, 'weight')
    
    return model

# Expected: 15-20% smaller with minimal accuracy loss
```

### 3. Multi-Worker Self-Play Coordination
```python
# Advanced worker distribution strategy

WORKER_CONFIGS = {
    1: {'opening_focus': True, 'temperature': 0.8},
    2: {'midgame_focus': True, 'temperature': 1.0},
    3: {'endgame_focus': True, 'temperature': 0.6},
    4: {'balanced': True, 'temperature': 0.9}
}

# Each worker specializes in different phases
# Balanced data collection across game phases
```

### 4. Dynamic Difficulty Scaling
```python
# Self-play with varied strength levels

def select_move_with_difficulty(board, strength=1.0):
    """
    strength: 0.5 (weak) to 1.0 (full strength)
    """
    if random.random() > strength:
        # Make suboptimal move
        legal_moves = list(board.legal_moves)
        return random.choice(legal_moves)
    else:
        return select_best_move(board)

# Benefits: More diverse training data, prevents overfitting
```

### 5. Ensemble Predictions (Production)
```python
# Use multiple model checkpoints for critical decisions

class EnsembleEngine:
    def __init__(self, model_paths):
        self.models = [ort.InferenceSession(p) for p in model_paths]
    
    def evaluate(self, board):
        evaluations = []
        for model in self.models:
            features = board_to_28_channels(board)
            output = model.run(None, {'board': features})
            evaluations.append(output[0][0])
        
        # Average or voting
        return np.mean(evaluations)

# Load last 3 successful versions for robustness
```

---

## üìä Monitoring & Analytics

### Training Metrics Dashboard
```python
# Track during training

METRICS = {
    'loss_total': [],
    'loss_value': [],
    'loss_policy': [],
    'learning_rate': [],
    'gradient_norm': [],
    'epoch': []
}

# Log to TensorBoard or Weights & Biases
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/synapse_base')

def log_metrics(epoch, metrics):
    writer.add_scalar('Loss/Total', metrics['loss_total'], epoch)
    writer.add_scalar('Loss/Value', metrics['loss_value'], epoch)
    writer.add_scalar('Loss/Policy', metrics['loss_policy'], epoch)
    writer.add_scalar('Learning_Rate', metrics['learning_rate'], epoch)
```

### Self-Play Statistics
```python
# Track in HF Space

STATS = {
    'games_played': 0,
    'avg_game_length': 0,
    'result_distribution': {'1-0': 0, '0-1': 0, '1/2-1/2': 0},
    'avg_time_per_move': 0,
    'errors': 0
}

# Upload stats periodically
def upload_stats():
    api.upload_file(
        path_or_fileobj="stats.json",
        path_in_repo=f"Worker_{WORKER_ID}/stats.json",
        repo_id="GambitFlow/SelfPlay-Data",
        repo_type="dataset"
    )
```

### Evaluation Tracking
```python
# Version performance history

VERSION_HISTORY = {
    'v1': {'elo': 2200, 'date': '2025-01-01'},
    'v2': {'elo': 2250, 'date': '2025-01-15', 'improved': True},
    'v3': {'elo': 2245, 'date': '2025-01-22', 'improved': False}
}

# Visualize improvement over time
import matplotlib.pyplot as plt

versions = list(VERSION_HISTORY.keys())
elos = [VERSION_HISTORY[v]['elo'] for v in versions]

plt.plot(versions, elos, marker='o')
plt.title('Synapse-Base ELO Progression')
plt.xlabel('Version')
plt.ylabel('Estimated ELO')
plt.grid(True)
plt.savefig('elo_progression.png')
```

---

## üö® Troubleshooting Guide

### Issue 1: Model Predicts Illegal Moves
**Symptoms:** ONNX output doesn't match legal moves

**Solutions:**
1. Add legal move masking layer:
   ```python
   def mask_illegal_moves(logits, board):
       legal_moves = list(board.legal_moves)
       legal_indices = [move_to_index(m) for m in legal_moves]
       
       mask = torch.full_like(logits, -1e9)
       mask[legal_indices] = 0
       
       return logits + mask
   ```

2. Post-process output to select only legal moves
3. Check if board encoding is correct (flipped ranks/files)

### Issue 2: Training Loss Not Decreasing
**Symptoms:** Loss plateaus or increases

**Solutions:**
1. Reduce learning rate (1e-4 ‚Üí 1e-5)
2. Check data quality (balanced phases, correct labels)
3. Add gradient clipping (may already have)
4. Inspect batch samples manually
5. Reduce model complexity (fewer residual blocks)

### Issue 3: Self-Play Games Are Too Short
**Symptoms:** Average game length < 20 moves

**Solutions:**
1. Check if model is overly aggressive (premature exchanges)
2. Add exploration temperature to move selection
3. Penalize early draws in training data
4. Verify opening book isn't causing quick repetitions

### Issue 4: HF Space Keeps Crashing
**Symptoms:** Space restarts frequently

**Solutions:**
1. Check memory usage (16GB limit)
2. Add error handling and recovery:
   ```python
   try:
       games_buffer.append(game)
   except Exception as e:
       print(f"Error: {e}")
       time.sleep(60)
       continue  # Don't crash
   ```
3. Reduce batch size (100 ‚Üí 50 games)
4. Monitor logs for specific errors

### Issue 5: ONNX Inference Too Slow
**Symptoms:** >200ms per position on 2 vCPU

**Solutions:**
1. Apply quantization (see optimization section)
2. Reduce residual blocks (15 ‚Üí 12)
3. Use ONNX graph optimization:
   ```python
   sess_options = ort.SessionOptions()
   sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
   session = ort.InferenceSession(model_path, sess_options)
   ```
4. Profile bottleneck layers, simplify if needed

---

## üéì Research Papers & Techniques to Explore

### Core Inspiration
1. **AlphaZero** (Silver et al., 2017)
   - MCTS + neural networks
   - Self-play reinforcement learning
   - We're using simplified version (no MCTS for speed)

2. **Leela Chess Zero** (LCZero)
   - Community-driven chess engine
   - Similar architecture to ours
   - Self-play data generation at scale

3. **MuZero** (Schrittwieser et al., 2020)
   - Model-based reinforcement learning
   - Could apply to planning in chess

### Advanced Techniques (Future)

1. **Attention Mechanisms**
   ```python
   # Add attention to capture long-range piece interactions
   class SelfAttention(nn.Module):
       def __init__(self, channels):
           super().__init__()
           self.query = nn.Conv2d(channels, channels//8, 1)
           self.key = nn.Conv2d(channels, channels//8, 1)
           self.value = nn.Conv2d(channels, channels, 1)
       
       def forward(self, x):
           # Flatten spatial dimensions
           q = self.query(x).flatten(2)
           k = self.key(x).flatten(2)
           v = self.value(x).flatten(2)
           
           # Attention scores
           attn = torch.softmax(q @ k.transpose(-1, -2), dim=-1)
           out = (attn @ v).reshape_as(x)
           
           return x + out
   ```

2. **Curriculum Learning**
   - Start with simple endgames (KPK)
   - Progress to complex positions
   - Gradually increase difficulty

3. **Adaptive Search Depth**
   ```python
   def dynamic_search_depth(board):
       # Deeper search in critical positions
       if is_tactical(board):
           return 4  # Deeper
       elif is_endgame(board):
           return 3
       else:
           return 2  # Normal
   ```

4. **Opening Book Learning**
   - Use neural networks to compress opening knowledge
   - Predict "book move probability" instead of fixed database

5. **Time Management**
   ```python
   class TimeManager:
       def __init__(self, total_time):
           self.remaining = total_time
           self.moves_made = 0
       
       def allocate_time(self, board):
           # More time in complex positions
           complexity = count_legal_moves(board)
           base_time = self.remaining / 40  # Assume 40 moves left
           
           if complexity > 35:
               return base_time * 1.5
           elif complexity < 10:
               return base_time * 0.5
           else:
               return base_time
   ```

---

## üìà Roadmap & Future Versions

### Synapse-Base (v1.0) - Current Plan
- ‚úÖ Foundation supervised training
- ‚úÖ Self-play infrastructure
- ‚úÖ Basic fine-tuning loop
- Target: 2200-2400 ELO

### Synapse-Edge (v2.0) - Next Iteration
- üîÑ Add attention layers for global reasoning
- üîÑ Curriculum learning from simple to complex
- üîÑ Better time management
- üîÑ Advanced search algorithms (simplified MCTS)
- Target: 2400-2600 ELO

### Synapse-Pro (v3.0) - Advanced
- üîÆ Transformer-based policy head
- üîÆ Multi-task learning (predict tactics, strategy, plans)
- üîÆ Human-like playing style options
- üîÆ Opening novelty generation
- Target: 2600-2800 ELO

### Synapse-Ultra (v4.0) - Research Frontier
- üöÄ Full MCTS integration
- üöÄ Model-based planning (MuZero style)
- üöÄ Multi-agent training (population-based)
- üöÄ Explainable AI (why it chose this move)
- Target: 2800+ ELO (compete with top engines)

---

## üéØ Execution Timeline Summary

| Week | Phase | Key Deliverables | Est. Hours |
|------|-------|------------------|------------|
| 1 | Data Prep | Opening DB v2, Match Data, Puzzles, Endgame | 80-100h |
| 2 | Model Dev | Architecture, Training, Export | 40-50h |
| 3 | Self-Play | HF Space setup, Utilities, Testing | 30-40h |
| 4 | Fine-Tuning | Pipeline, Evaluation, Documentation | 40-50h |
| **Total** | | | **190-240h** |

**Parallel Execution Opportunities:**
- Data collection notebooks can run simultaneously (4 parallel)
- Self-play workers run in parallel (infinite)
- Documentation written during compute-heavy tasks

**Real-World Timeline:**
- With dedicated focus: 4-6 weeks
- Part-time work: 8-12 weeks
- Continuous improvement: Ongoing

---

## üé¨ Getting Started - First Steps

### Day 1 - Immediate Actions
```bash
# 1. Set up HF repositories
# Create these repos manually on HuggingFace:
# - GambitFlow/Elite-Data (dataset)
# - GambitFlow/SelfPlay-Data (dataset)
# - GambitFlow/Synapse-Base (model)

# 2. Clone this plan to your Drive
# Create folder: /content/drive/MyDrive/GambitFlow_Project

# 3. Start with Opening DB v2
# Open: 01_Opening_Architect_v2.ipynb
# Run cells 1-2 to download data
# Let it run overnight (15-20 hours)

# 4. Meanwhile, review architecture
# Read: models/synapse_base.py
# Understand the 28-channel input design

# 5. Prepare for Week 2
# Ensure you have Colab Pro (for longer runtimes)
# Or split training across multiple free sessions
```

### Quick Validation Checklist
```python
# Before starting full training, validate:

# ‚úÖ Can you encode a board to 28 channels?
test_board = chess.Board()
tensor = board_to_28_channels(test_board)
assert tensor.shape == (28, 8, 8)

# ‚úÖ Can you load data from HF?
db_path = hf_hub_download(repo_id="GambitFlow/Elite-Data", filename="test.db")
assert os.path.exists(db_path)

# ‚úÖ Can model forward pass work?
model = SynapseBase()
output = model(torch.randn(1, 28, 8, 8))
assert len(output) == 3  # value, opening, tactical

# ‚úÖ Can you export to ONNX?
torch.onnx.export(model, torch.randn(1, 28, 8, 8), "test.onnx")
assert os.path.exists("test.onnx")

print("‚úÖ All validations passed! Ready to start.")
```

---

## üìö Essential Resources

### Documentation to Write
1. **ARCHITECTURE.md**: Detailed model design rationale
2. **DATA_PIPELINE.md**: How each dataset is constructed
3. **TRAINING_GUIDE.md**: Step-by-step training instructions
4. **DEPLOYMENT.md**: How to deploy on HF Spaces
5. **API_REFERENCE.md**: Using the inference endpoint

### Code Comments Standard
```python
# Use this format for all functions:

def function_name(param1, param2):
    """
    Brief description of what this function does.
    
    Args:
        param1 (type): Description
        param2 (type): Description
    
    Returns:
        type: Description
    
    Example:
        >>> function_name(1, 2)
        3
    """
    pass
```

### Testing Strategy
```python
# Unit tests for critical functions

def test_board_encoding():
    board = chess.Board()
    tensor = board_to_28_channels(board)
    
    # Test piece placement
    assert tensor[4, 7, 3] == 1.0  # White queen on d1
    assert tensor[10, 0, 3] == 1.0  # Black queen on d8
    
    # Test turn
    assert tensor[12, 0, 0] == 1.0  # White to move
    
    print("‚úÖ Board encoding test passed")

def test_illegal_move_filtering():
    board = chess.Board("8/8/8/8/8/8/8/K6k w - - 0 1")
    moves = get_legal_moves(board)
    
    assert len(moves) == 3  # Ka2, Kb1, Kb2
    print("‚úÖ Legal move filtering test passed")

# Run all tests before deployment
test_board_encoding()
test_illegal_move_filtering()
```

---

## üèÜ Success Metrics & KPIs

### Technical Metrics
- **Model Size**: 20-30 MB (Target: 25 MB) ‚úÖ
- **Inference Latency**: <100ms on 2 vCPU (Target: 60-80ms) ‚úÖ
- **Training Loss**: <0.05 final MSE ‚úÖ
- **Legal Move Accuracy**: 100% ‚úÖ

### Chess Performance Metrics
- **Tactical Puzzle Rating**: 2000+ (Target: 2200+) üéØ
- **Lichess Blitz Rating**: 2200+ (Target: 2400+) üéØ
- **Opening Accuracy**: 90%+ book compliance üéØ
- **Endgame Win Rate**: 95%+ in won positions üéØ

### Development Metrics
- **Self-Play Games Generated**: 10,000+ per month üìä
- **Fine-Tuning Cycles**: 1 per week üîÑ
- **Model Improvement Rate**: +50 ELO per generation üìà
- **Space Uptime**: 95%+ availability ‚è±Ô∏è

### Community Metrics (Optional)
- **GitHub Stars**: Track interest üåü
- **HF Model Downloads**: Measure adoption üì•
- **API Requests**: Usage tracking üìä
- **Community Contributions**: PRs, issues ü§ù

---

## üéì Learning Outcomes

By completing this project, you will master:

### Machine Learning
- ‚úÖ Deep learning architecture design
- ‚úÖ Multi-task learning (value + policy)
- ‚úÖ Reinforcement learning (self-play)
- ‚úÖ Model compression & optimization
- ‚úÖ ONNX export & deployment

### Chess AI
- ‚úÖ Position encoding strategies
- ‚úÖ Opening theory databases
- ‚úÖ Tactical pattern recognition
- ‚úÖ Endgame tablebase integration
- ‚úÖ Search algorithms

### MLOps
- ‚úÖ Data pipeline design
- ‚úÖ Continuous training loops
- ‚úÖ Model versioning & promotion
- ‚úÖ Cloud deployment (HF Spaces)
- ‚úÖ Monitoring & analytics

### Software Engineering
- ‚úÖ Large-scale project organization
- ‚úÖ Documentation best practices
- ‚úÖ Testing strategies
- ‚úÖ API design
- ‚úÖ Performance optimization

---

## üåü Vision Statement

**Synapse-Base** aims to be:

1. **Accessible**: Runs on modest hardware (2 vCPU, 16GB RAM)
2. **Transparent**: Open-source, well-documented, explainable
3. **Improving**: Self-play loop ensures continuous enhancement
4. **Practical**: Fast inference (<100ms) for real-time play
5. **Competitive**: Target 2400+ ELO, rivaling commercial engines

**Core Philosophy:**
> "Not the strongest engine, but the smartest use of limited resources."

We prioritize:
- Efficiency over raw power
- Knowledge distillation over brute force
- Elegant architecture over complexity
- Community collaboration over proprietary secrecy

---

## üìû Support & Collaboration

### Getting Help
1. Check documentation first (ARCHITECTURE.md, etc.)
2. Review troubleshooting guide (above)
3. Search existing GitHub issues
4. Post detailed question with error logs

### Contributing
1. Fork the repository
2. Create feature branch
3. Make changes with tests
4. Submit pull request with description

### Reporting Issues
Use this template:
```markdown
**Issue**: Brief description
**Environment**: Colab/Local, GPU/CPU, Python version
**Steps to Reproduce**: 
1. Step 1
2. Step 2
**Expected**: What should happen
**Actual**: What actually happens
**Logs**: Error messages (paste here)
```

---

## üéâ Conclusion

This comprehensive plan provides everything needed to build **Synapse-Base** from scratch:

‚úÖ **Data Pipeline**: 4 specialized datasets (opening, match, puzzle, endgame)  
‚úÖ **Model Architecture**: Lightweight 28-channel CNN (20-30 MB)  
‚úÖ **Training Strategy**: Supervised + self-play reinforcement  
‚úÖ **Deployment**: HF Spaces with automated fine-tuning  
‚úÖ **Evaluation**: Tournament system with model promotion  

**Total Project Scope**: 190-240 hours over 4-6 weeks

**Expected Result**: A competitive chess engine (2200-2400 ELO) that runs efficiently on CPU and continuously improves through self-play.

---

## üìù NOTES: Critical Corrections from Analysis

**Based on user feedback, the following corrections have been made to the plan:**

### ‚ùå REMOVED: Stockfish Integration
- **Original**: Week 4 included knowledge distillation from Stockfish
- **Correction**: This violates project constraints (no external engine evaluations)
- **New Approach**: Rely entirely on supervised learning from game outcomes + self-play improvement

### ‚úÖ CORRECTED: Endgame Worker Distribution
- **Original Plan Assumption**: Workers 1-3 generate random endgame positions
- **Actual Implementation** (from notebook analysis):
  - Worker 1: 3-piece endgames only
  - Worker 2: 4-piece endgames only  
  - Worker 3: 5-piece endgames only
- **Implication**: Current 600K positions (3 workers √ó 200K) are well-distributed across piece counts
- **Recommendation**: Add 3 more workers (4-6) with same distribution to reach 1.2M total positions

### ‚úÖ CONFIRMED: Data Sources
All training labels come from:
1. **Opening**: Human game outcomes (ELO 2600+)
2. **Midgame/Tactics**: Human game outcomes + puzzle solutions
3. **Endgame**: Syzygy tablebase (perfect play)
4. **Self-Play**: Model's own games (reinforcement signal)

**No external engine evaluations are used anywhere in the pipeline.**

### ‚ö†Ô∏è CLARIFIED: Fine-Tuning Data Source
- Self-play games provide reinforcement signal
- Model learns from its own mistakes and successes
- No ground truth from external engines
- Evaluation tournament determines if improvement is real

### üéØ FINAL ARCHITECTURE DECISION
- Keep 28-channel input (not 119)
- 15 residual blocks (proven sweet spot)
- No Transformer (too slow for 2 vCPU)
- Dual policy heads (opening book + tactical)
- Target: 25 MB ONNX, <80ms inference

**All notebooks in this plan are now fully self-contained and do not rely on external chess engines.**

---

**END OF PLAN.MD**

---

## üìä SUMMARY

This master plan provides a complete roadmap for building **GambitFlow Synapse-Base**, a lightweight yet powerful chess engine optimized for CPU inference. The plan includes:

- **7 data preparation notebooks** (opening, match games, puzzles, endgame)
- **3 training notebooks** (architecture, initial training, fine-tuning)
- **2 evaluation notebooks** (ONNX export, tournament system)
- **1 HF Spaces application** (self-play worker with automated data upload)
- **Complete self-improvement loop** (generate games ‚Üí fine-tune ‚Üí evaluate ‚Üí promote)

**Key Innovation**: No external engine dependencies - all knowledge comes from curated human games, puzzles, tablebases, and self-play.

**Time Investment**: 190-240 hours (4-6 weeks full-time, 8-12 weeks part-time)

**Expected Outcome**: Competitive 2200-2400 ELO chess engine running at <100ms per move on 2 vCPU.
